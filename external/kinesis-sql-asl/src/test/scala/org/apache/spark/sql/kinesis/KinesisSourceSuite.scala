/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.kinesis

import scala.util.Random

import org.scalatest.time.SpanSugar._

import org.apache.spark.sql.execution.streaming.{Offset, Source, StreamExecution, StreamingExecutionRelation}
import org.apache.spark.sql.streaming.StreamTest
import org.apache.spark.sql.types._
import org.apache.spark.streaming.kinesis.{KinesisFunSuite, KinesisTestUtils, KPLBasedKinesisTestUtils}

abstract class KinesisSourceTests(aggregateTestData: Boolean) extends StreamTest
    with KinesisFunSuite {

  import testImplicits._

  // This is the name that KCL will use to save metadata to DynamoDB
  private val appName = s"KinesisSourceTests-${math.abs(Random.nextLong())}"

  private var testUtils: KinesisTestUtils = _

  override val streamingTimeout = 360.seconds

  override def beforeAll(): Unit = {
    super.beforeAll()
    runIfTestsEnabled("Prepare KinesisTestUtils") {
      // TODO: How to make sure that we insert data into multiple shards?
      testUtils = new KPLBasedKinesisTestUtils(1)
      testUtils.createStream()
    }
  }

  override def afterAll(): Unit = {
    if (testUtils != null) {
      // Delete the Kinesis stream as well as the DynamoDB table generated by
      // Kinesis Client Library when consuming the stream
      testUtils.deleteStream()
      testUtils.deleteDynamoDBTable(appName)
    }
    super.afterAll()
  }

  protected def makeSureGetOffsetCalled = AssertOnQuery { q =>
    // Because KinesisSource's initialShardSeqNumbers is set lazily, we need to make sure
    // its "getOffset" is called before pushing any data. Otherwise, because of the race condition,
    // we don't know which data should be fetched when `startingOffset` is latest.
    q.processAllAvailable()
    true
  }

  // TODO: Support multiple streams, `AddKinesisData(streams: Seq[String], data: Int*)`
  case class AddKinesisData(data: Int*) extends AddData {

    override def addData(query: Option[StreamExecution]): (Source, Offset) = {
      val kinesisSource = query.map {
        case q =>
          val sources = q.logicalPlan.collect {
            case StreamingExecutionRelation(source, _) if source.isInstanceOf[KinesisSource] =>
              source.asInstanceOf[KinesisSource]
          }
          if (sources.isEmpty) {
            throw new Exception(
              "Could not find Kinesis source in the StreamExecution logical plan to add data to")
          } else if (sources.size > 1) {
            throw new Exception(
              "Could not select the Kinesis source in the StreamExecution logical plan as there" +
                "are multiple Kinesis sources:\n\t" + sources.mkString("\n\t"))
          }
          sources.head
      }.getOrElse {
        throw new RuntimeException(
          "Cannot add data when there is no query for finding the active Kinesis source")
      }

      // Push data into the a shard of Kinesis streams
      val sentMetadata = testUtils.pushData(data, aggregateTestData)

      val offsetTuples = sentMetadata.map { case (shardId, metadataSeq) =>
        (testUtils.streamName, shardId, metadataSeq.map(_._2).max)
      }
      (kinesisSource, KinesisSourceOffset(offsetTuples.toSeq: _*))
    }

    override def toString: String =
      s"AddKinesisData(stream = ${testUtils.streamName}, data = $data)"
  }

  testIfEnabled("consume test stream from the latest sequence numbers in shards") {
    val kinesis = spark
      .readStream
      .format("kinesis")
      .schema(StructType(StructField("value", IntegerType) :: Nil))
      .option("streams", testUtils.streamName)
      .option("endpointUrl", testUtils.endpointUrl)
      .option("regionName", testUtils.regionName)
      .option("checkpointName", appName)
      .option("format", "csv")
      .option("InitialPositionInStream", "TRIM_HORIZON")
      .option("retryTimeoutMs", "10000")
      .option("failOnDataLoss", "false")
      .load

    testStream(kinesis.select(kinesis.col("value")))(
      AddKinesisData(1, 2, 3),
      CheckAnswer(1, 2, 3),
      StopStream,
      StartStream(),
      CheckAnswer(1, 2, 3),
      AddKinesisData(4, 5),
      CheckAnswer(1, 2, 3, 4, 5)
    )
  }
}

class WithAggregationKinesisSourceSuite extends KinesisSourceTests(aggregateTestData = true)

class WithoutAggregationKinesisSourceSuite extends KinesisSourceTests(aggregateTestData = false)
